{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workshop: Generating Text using an N-gram Model\n",
    "\n",
    "## What is an N-gram Model?\n",
    "\n",
    "An $N$-gram model is a simple but powerful tool used in natural language processing (NLP) for predicting the next word in a sequence of words. It's based on the idea that the probability of a word occurring depends on the previous $N-1$ words in the sequence.\n",
    "\n",
    "## How Does it Work?\n",
    "\n",
    "Imagine we have a sentence: \"The quick brown fox jumps\". \n",
    "\n",
    "- A unigram model ($N=1$) predicts the next word based on the probability of individual words. For example, given \"The\", it might predict \"quick\" with high probability because \"quick\" often follows \"The\".\n",
    "- A bigram model ($N=2$) predicts the next word based on the probability of pairs of words. For example, given \"quick brown\", it might predict \"fox\" because \"fox\" often follows \"quick brown\".\n",
    "- An $N$-gram model generalizes this concept to predict the next word based on the probability of the previous $N-1$ words.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "In our workshop, we'll create a simple $N$-gram model using PyTorch. Here's a simplified overview of the architecture:\n",
    "\n",
    "1. **Tokenization**: We'll convert our text data into tokens (words or characters).\n",
    "2. **Embedding**: Each token is represented as a high-dimensional vector (embedding).\n",
    "3. **Concatenation**: We concatenate the embeddings of the previous $N-1$ tokens into a single vector.\n",
    "4. **Fully-Connected Layers**: We pass the concatenated vector through fully-connected layers with activation functions like ReLU.\n",
    "5. **Output Layer**: Finally, we predict the probability distribution over the vocabulary to determine the next token.\n",
    "\n",
    "## Workshop Goals\n",
    "\n",
    "By the end of this workshop, you'll understand the basics of $N$-gram models and how they can be implemented using PyTorch. You'll also be able to train your own model and generate text based on the patterns it learns from the training data.\n",
    "\n",
    "Let's dive in and explore the world of NLP with $N$-gram models!\n",
    "\n",
    "---\n",
    "\n",
    "*Author: Shivam Gupta*\n",
    "\n",
    "*GitHub Repository: [shivamCode0/ngram](https://github.com/shivamCode0/ngram)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
