{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Data path set to: data\n",
      "[DEBUG] Initializing tokenizer and model\n",
      "[DEBUG] Tokenizer and model initialized\n",
      "[DEBUG] Loading dataset from data\\latex_data.txt\n",
      "[DEBUG] Dataset loaded with 160 samples\n",
      "<transformers.data.datasets.language_modeling.TextDataset object at 0x000001D5985A27B0> data\\latex_data.txt\n",
      "[DEBUG] Data collator prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\miniconda3\\envs\\ml2\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Trainer initialized\n",
      "[DEBUG] Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4cee2140e04354b34c8a7f91e00cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 59.9065, 'train_samples_per_second': 8.012, 'train_steps_per_second': 8.012, 'train_loss': 2.7125673929850262, 'epoch': 3.0}\n",
      "[DEBUG] Training completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "\n",
    "def debug_message(message):\n",
    "    print(f\"[DEBUG] {message}\")\n",
    "\n",
    "# Set data path\n",
    "data_path = \"data\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "debug_message(f\"Data path set to: {data_path}\")\n",
    "\n",
    "# # Fetching LaTeX content from your dataset (dummy content for example)\n",
    "# latex_texts = [\n",
    "#     \"This is an example of an equation: \\\\begin{equation} E = mc^2 \\\\end{equation}\",\n",
    "#     \"Here is a figure: \\\\begin{figure} \\\\includegraphics{example.png} \\\\caption{An example figure} \\\\end{figure}\",\n",
    "#     \"An itemized list: \\\\begin{itemize} \\\\item First item \\\\item Second item \\\\end{itemize}\",\n",
    "#     \"A table: \\\\begin{table} \\\\begin{tabular}{c c} a & b \\\\end{tabular} \\\\end{table}\"\n",
    "# ]\n",
    "\n",
    "# debug_message(\"Fetched LaTeX content from dataset\")\n",
    "\n",
    "# # Save your LaTeX texts to a file\n",
    "latex_file_path = os.path.join(data_path, 'latex_data.txt')\n",
    "\n",
    "# with open(latex_file_path, 'w') as f:\n",
    "#     for text in tqdm(latex_texts, desc=\"Saving LaTeX texts\"):\n",
    "#         f.write(text + '\\n')\n",
    "# debug_message(f\"LaTeX texts saved to {latex_file_path}\")\n",
    "\n",
    "# Tokenizer and model initialization\n",
    "\n",
    "debug_message(\"Initializing tokenizer and model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "debug_message(\"Tokenizer and model initialized\")\n",
    "\n",
    "# Prepare dataset\n",
    "def load_dataset(file_path, tokenizer):\n",
    "\n",
    "    debug_message(f\"Loading dataset from {file_path}\")\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=64  # Adjust as needed\n",
    "    )\n",
    "    debug_message(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(latex_file_path, tokenizer)\n",
    "print(dataset,latex_file_path)\n",
    "\n",
    "# Prepare data collator\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "debug_message(\"Data collator prepared\")\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Adjust based on your GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "# debug_message(f\"Training arguments set: {training_args}\")\n",
    "\n",
    "# Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "debug_message(\"Trainer initialized\")\n",
    "\n",
    "# Start training\n",
    "\n",
    "debug_message(\"Starting training\")\n",
    "trainer.train()\n",
    "debug_message(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model saved to data\\model\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_path = os.path.join(data_path, 'model')\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "debug_message(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "debug_message(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~$ \\osk eqn_{0}(~2)$ is a measure of the strength of regimes, not of the total number of atoms in the whole system. \n",
      "~\\begin{equation}\n",
      "\\label{fig3}\n",
      "\\-----\n",
      "\\end{figure}\n",
      "\\ref{fig2}\n",
      "In the case of the classical classical quantum energy $F(t)=\\sum_{i=0}^2 + \\sum_{j=0}^{i-j(t)}^2 \\approx{n^2\\omega_{t}$ where t is the time of state evolution, \n",
      "j and t the initial perturbation. In the case of perturbations in the bosons of $t=0$ the dynamics of the corresponding quantum state is perturbed by the coupling of two charged particles. \n",
      "This perturbation in the classical regime is due to the energy (or volume) $t=0$ (\\sum_{j=0}^2}^2+\\sum_{j=0}^{i-j(t)}^2$, \n",
      "which is a measure of the initial fidelity at position $t=0$. In the classical regime the above perturbations are seen in fig1~\\ref{fig4}\n",
      "while in the classical regime the perturbation is due to the interaction between two bosons. \n",
      "In the classical regime $F(t)=\\sum_{j=0}^2+\\sum_{j=0};$ the perturbation in the optical regime is perturbed by the coupling of two \n",
      "states of the classical regime \n",
      "$k=0$. In the classical regime the two bosons are charged at constant energies $t=0$, while the coupling is perturbed by the energy (or volume) $t=1$. In the classical regime $F(t)=\\sum_{j=0}^{i-j(t)}^2-(t)}$. \n",
      "In the classical regime $F(t)=\\sum_{j~\\textbf{c}}(t)}^2-(t)}$. \n",
      "\\bibitem{GZ06}\n",
      "\\end{figure}\n",
      "The two regimes at the same time have different strengths, some due to"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     30\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m---> 31\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Append the token id to the output sequence\u001b[39;00m\n\u001b[0;32m     34\u001b[0m output_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((output_sequences, next_token_id), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Generate text\n",
    "prompt = \"\"\"\\\n",
    "TITLE: Riemann Hypothesis\n",
    "What is the Riemann Hypothesis?\n",
    "\"\"\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model and inputs to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Generate tokens one by one\n",
    "output_sequences = input_ids\n",
    "generated_sequence = []\n",
    "\n",
    "for _ in range(1000):  # Adjust the number of tokens to generate\n",
    "    # Get the model's output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(output_sequences)\n",
    "    \n",
    "    # Get the next token logits and find the most probable token\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    \n",
    "    temperature = 0.9\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "    next_token_id = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "    \n",
    "    # Append the token id to the output sequence\n",
    "    output_sequences = torch.cat((output_sequences, next_token_id), dim=1)\n",
    "    \n",
    "    # Decode the token and print it\n",
    "    generated_token = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "    generated_sequence.append(generated_token)\n",
    "    print(generated_token, end='', flush=True)\n",
    "\n",
    "# Join all tokens to form the final generated text\n",
    "generated_text = \"\".join(generated_sequence)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
